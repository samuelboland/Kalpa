{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elder Scrolls Normal Map Generation via CycleGAN\n",
    "Using implementation of CycleGAN here: https://github.com/LynnHo/CycleGAN-Tensorflow-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this document is to explain how to utilize CycleGANs to generate normal maps for input texture for The Elder Scrolls III: Morrowind's open-source engine reimplementation, OpenMW. \n",
    "\n",
    "This document is intended for advanced users. If you are not comfortable in the command line and in troubleshooting issues on your own, then this guide is not currently intended for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion of Network Architecture\n",
    "\n",
    "First, let me be clear that I don't really understand how this network works under the hood. It's a tool for me. In the same way that I don't understand the details of how video cards work, or how microwaves work, I don't really know how this works. However, I am learning, and will continue to learn. \n",
    "\n",
    "If you do, please, add to this and enhance it! \n",
    "\n",
    "The purpose of this network is to generate Normal/Height maps from arbitrary input textures. To do this, we need to train the network on how to translate from an input texture to the desired normal/height map. We accomplish this with a CycleGAN. CycleGANs are a relatively new form of Deep Learning network that does not require paired data. We simply give it a selection of normals, and a selection of regular textures, and it learns how to translate back and forth between them. They are a fascinating subject, and there are some good resources around the internet explaining them.\n",
    "\n",
    "The choice of input data is very important. We can't just take all of the normal/heightmaps in the broader Elder Scrolls modding world and dump them into a folder. (I did try that though.) We need to train the network on more specific groupings. \n",
    "\n",
    "My current attempt is to train this network with architecture - specifically, large relatively flat surfaces with mostly rectilinear components, such as tile floor or stone walls. I am using a selection Lysol's wonderful normalmaps (which you can find here: https://www.nexusmods.com/morrowind/users/34241390 ) with his permission to train this network. \n",
    "\n",
    "My first attempt ignored the alpha channel on normal maps, which proved to be a bad decision - a lot of data is encoded in that channel. This journal contains code the split the input normals into their component RGBA channels - Red, Green, Blue, and Alpha. It then resizes them to the necessary dimensions, moves them to the necessary locations, and returns a list of commands for you to run in the command prompt. \n",
    "\n",
    "After the networks have been trained on each of the channels, we take input textures and run them through each network. Finally, we combine the resulting single-channel normals into a single image, resize it to the original values, and - hopefully - we are done. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0 - Preliminary Setup\n",
    "\n",
    "First, get Tensorflow running on your computer. The official installation instructions are here: https://www.tensorflow.org/install/gpu. It is strongly recommended to use a GPU for this, which requires a modern Nvidia GPU. I am using an RTX 2060 non-super with 6GB VRAM - and I wish I had more.\n",
    "\n",
    "Second, download the CycleGAN implementation listed above. Ensure that you have it working by training it on the test dataset mentioned in the repository. \n",
    "\n",
    "Place the folder containing the GAN in the folder with this notebook. I suggest creating a new folder for this purpose, as we will be creating many files and directories. If this is on your desktop or in your downloads folder, it will get messy.\n",
    "\n",
    "Finally, install Imagemagick. https://imagemagick.org/index.php We will use this to mass-convert the finalized images to dds for easy use in Morrowind, Oblivion, or Skyrim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Dataset Preprocessing and Segmentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network operates on 256x256 square images. We need to adjust our images to fit this requirement. \n",
    "\n",
    "To accomplish this, I have written the following script that will take images in the subdirectory \"Raw_Input\" and convert them to the required size and format and store them in the \"Processed_Input\" directory.\n",
    "    \n",
    "You should have one set of folders each for Diffuse and Normals.\n",
    "\n",
    "In the following blocks, we split the Heightmaps into four separate images - one for each channel R, G, B, and A. We will train four networks, one for each component, and then recombine the final images at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from PIL import ImageEnhance\n",
    "from PIL import ImageOps\n",
    "\n",
    "GAN_location = \"./CycleGAN-Tensorflow-2-master/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Using pre-existing resize directory ======\n",
      "====== Using pre-existing resize directory ======\n",
      "====== Using pre-existing resize directory ======\n",
      "====== Using pre-existing resize directory ======\n",
      "Processing: 500wall.dds\n",
      "Processing: cpstone01.dds\n",
      "Processing: cpstone02.dds\n",
      "Processing: cpstone03.dds\n",
      "Processing: cpstone05.dds\n",
      "Processing: cpstone06.dds\n",
      "Processing: markarthtemp02.dds\n",
      "Processing: mrkrockdesigns01.dds\n",
      "Processing: mrkstoneblocks03.dds\n",
      "Processing: mrktileslates01.dds\n",
      "Processing: mrkwalkwaysa01.dds\n",
      "Processing: skyhavenfloorstones01.dds\n",
      "Processing: skyhavenstones01.dds\n",
      "Processing: sovroof01.dds\n",
      "Processing: sovstonestacked01.dds\n",
      "Processing: tr_nec_floor.dds\n",
      "Processing: tr_nec_floor.jpg\n",
      "Processing: tr_nec_wall3_01.dds\n",
      "Processing: tx_brickedge_left_01.dds\n",
      "Processing: tx_brickedge_right_01.dds\n",
      "Processing: tx_colony_floor03.jpg\n",
      "Processing: tx_common_stone_floor.dds\n",
      "Processing: tx_common_stone_floor.jpg\n",
      "Processing: tx_dae_floor_01.jpg\n",
      "Processing: tx_dwe_brick00.dds\n",
      "Processing: tx_imp_floor_01.jpg\n",
      "Processing: tx_imp_wall_02.dds\n",
      "Processing: tx_redoran_floor_01.jpg\n",
      "Processing: Tx_Rough_Stone_Wall.dds\n",
      "Processing: tx_rough_stone_wall02.dds\n",
      "Processing: tx_sewer_wall_02.dds\n",
      "Processing: tx_stone_wall_02.dds\n",
      "Processing: tx_s_rough_stone_wall.dds\n",
      "Processing: tx_telv_floor_int_03.jpg\n",
      "Processing: tx_v_floor_01.jpg\n",
      "Processing: tx_wall_brick_imp_01.dds\n",
      "Processing: tx_wg_cobblestones_01.jpg\n",
      "Processing: tx_wg_cobblestone_01.jpg\n",
      "Processing: wallstone01.dds\n",
      "Processing: wallstone02.dds\n",
      "Processing: wallstone13.dds\n",
      "Processing: wallstone14.dds\n",
      "Processing: wallstone17.dds\n",
      "Processing: wallstoneb06.dds\n",
      "Processing: wallwood01.dds\n",
      "Processing: wallwood03.dds\n",
      "Processing: whdiamondtile2.dds\n",
      "Processing: whdirtbrick.dds\n",
      "Processing: whruinstonedark.dds\n",
      "Processing: whshingle01.dds\n",
      "Processing: whwoodbaselight.dds\n",
      "Processing: whwoodbeams.dds\n",
      "Processing: whwoodboards01.dds\n",
      "Processing: whwoodboards02.dds\n",
      "Processing: winterholdfloor02.dds\n",
      "Processing: winterholdgate01.dds\n",
      "Processing: winterholdwall02.dds\n",
      "Processing: wrdragontile01.dds\n",
      "Processing: 500wall_n.dds\n",
      "Processing: arenastonecolumn01bloody_n.dds\n",
      "Processing: arenastonecolumn01_n.dds\n",
      "Processing: arenastonegrey01_n.dds\n",
      "Processing: arenastonegrey02_n.dds\n",
      "Processing: arenastonegrey04_n.dds\n",
      "Processing: arenawoodwall01_n.dds\n",
      "Processing: cobblestonecheydinhal03_N.DDS\n",
      "Processing: cpstone02_N.DDS\n",
      "Processing: cpstone05_N.DDS\n",
      "Processing: cpstone06_N.DDS\n",
      "Processing: glasswindow10lowerclass_n.dds\n",
      "Processing: glasswindow10middleclass_n.dds\n",
      "Processing: glasswindow10_n.dds\n",
      "Processing: hhcol01_n.dds\n",
      "Processing: hhcol02_n.dds\n",
      "Processing: hhcol03_n.dds\n",
      "Processing: hhsteps01_n.dds\n",
      "Processing: markarthtemp02_n.dds\n",
      "Processing: mrkrockdesigns01_n.dds\n",
      "Processing: mrkstoneblocks03_n.dds\n",
      "Processing: mrktileslates01_n.dds\n",
      "Processing: mrkwalkwaysa01_n.dds\n",
      "Processing: riftenplazabrick01_n.dds\n",
      "Processing: riftenplazabrick02_n.dds\n",
      "Processing: riftenroofshingles01_n.dds\n",
      "Processing: riftenstonewall01_n.dds\n",
      "Processing: roof01_n.dds\n",
      "Processing: shipwoodside02_n.dds\n",
      "Processing: shipwoodside03_n.dds\n",
      "Processing: skyhavenfloorstones01_n.dds\n",
      "Processing: skyhavenroof01_n.dds\n",
      "Processing: sovfloor02_n.dds\n",
      "Processing: sroofslate01_n.dds\n",
      "Processing: stonewall01_n.dds\n",
      "Processing: thatch02_n.dds\n",
      "Processing: thatchinterior01_n.dds\n",
      "Processing: tx_ai_mainroad_01_nh.dds\n",
      "Processing: tx_bc_muck_nh.dds\n",
      "Processing: tx_brickedge_left_01_nh.dds\n",
      "Processing: tx_brickedge_right_01_nh.dds\n",
      "Processing: tx_common_door_nh.dds\n",
      "Processing: tx_common_stone_floor_nh.dds\n",
      "Processing: tx_doorbricks_01_nh.dds\n",
      "Processing: tx_imp_floor_02_nh.dds\n",
      "Processing: tx_imp_full_01_nh.dds\n",
      "Processing: tx_imp_half_01_nh.dds\n",
      "Processing: tx_imp_wall_01_nh.dds\n",
      "Processing: tx_imp_wall_02_nh.dds\n",
      "Processing: tx_imp_wall_03_nh.dds\n",
      "Processing: tx_shingles_01_nh.dds\n",
      "Processing: tx_v_base_09_nh.dds\n",
      "Processing: tx_v_base_10_nh.dds\n",
      "Processing: tx_v_bridgedetail_01_nh.dds\n",
      "Processing: tx_v_bridgedetail_04_nh.dds\n",
      "Processing: tx_v_floor_01_nh.dds\n",
      "Processing: tx_v_strip_02_nh.dds\n",
      "Processing: tx_wall_brick_imp_01_nh.dds\n",
      "Processing: tx_wall_stone_01_nh.dds\n",
      "Processing: tx_wg_cobblestones_01_nh.dds\n",
      "Processing: tx_wg_cobblestone_01_nh.dds\n",
      "Processing: tx_wood_ceilingpanel_02_nh.dds\n",
      "Processing: tx_wood_wornfloor_01_nh.dds\n",
      "Processing: whdirtbrick_n.dds\n",
      "Processing: whshingle01_n.dds\n",
      "Processing: whstreetstone01_n.dds\n",
      "Processing: winterholdfloor01_n.dds\n",
      "Processing: winterholdfloor02_n.dds\n",
      "Processing: winterholdgate01_n.dds\n",
      "Processing: winterholdwall02_n.dds\n",
      "Processing: woodwall01_n.dds\n",
      "Processing: wrcitywall01_n.dds\n",
      "Processing: wrdragontile01_n.dds\n",
      "Processing: wrfloorboards01_n.dds\n",
      "Processing: wrtempleboards01_n.dds\n",
      "Processing: wrtempletiles01_n.dds\n",
      "Processing: wrwoodfloorint01_n.dds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['500wall_n', 4096, 4096],\n",
       " ['arenastonecolumn01bloody_n', 2048, 2048],\n",
       " ['arenastonecolumn01_n', 2048, 2048],\n",
       " ['arenastonegrey01_n', 2048, 2048],\n",
       " ['arenastonegrey02_n', 2048, 2048],\n",
       " ['arenastonegrey04_n', 2048, 2048],\n",
       " ['arenawoodwall01_n', 2048, 2048],\n",
       " ['cobblestonecheydinhal03_N', 512, 512],\n",
       " ['cpstone02_N', 512, 512],\n",
       " ['cpstone05_N', 512, 512],\n",
       " ['cpstone06_N', 512, 512],\n",
       " ['glasswindow10lowerclass_n', 256, 512],\n",
       " ['glasswindow10middleclass_n', 256, 512],\n",
       " ['glasswindow10_n', 256, 512],\n",
       " ['hhcol01_n', 4096, 4096],\n",
       " ['hhcol02_n', 4096, 4096],\n",
       " ['hhcol03_n', 4096, 4096],\n",
       " ['hhsteps01_n', 4096, 4096],\n",
       " ['markarthtemp02_n', 2048, 2048],\n",
       " ['mrkrockdesigns01_n', 2048, 2048],\n",
       " ['mrkstoneblocks03_n', 2048, 2048],\n",
       " ['mrktileslates01_n', 2048, 2048],\n",
       " ['mrkwalkwaysa01_n', 4096, 4096],\n",
       " ['riftenplazabrick01_n', 4096, 4096],\n",
       " ['riftenplazabrick02_n', 2048, 2048],\n",
       " ['riftenroofshingles01_n', 1024, 1024],\n",
       " ['riftenstonewall01_n', 2048, 2048],\n",
       " ['roof01_n', 1024, 1024],\n",
       " ['shipwoodside02_n', 4096, 4096],\n",
       " ['shipwoodside03_n', 2048, 2048],\n",
       " ['skyhavenfloorstones01_n', 4096, 4096],\n",
       " ['skyhavenroof01_n', 1024, 1024],\n",
       " ['sovfloor02_n', 4096, 4096],\n",
       " ['sroofslate01_n', 2048, 2048],\n",
       " ['stonewall01_n', 2048, 2048],\n",
       " ['thatch02_n', 1024, 1024],\n",
       " ['thatchinterior01_n', 1024, 1024],\n",
       " ['tx_ai_mainroad_01_nh', 2048, 2048],\n",
       " ['tx_bc_muck_nh', 2048, 2048],\n",
       " ['tx_brickedge_left_01_nh', 128, 1024],\n",
       " ['tx_brickedge_right_01_nh', 128, 1024],\n",
       " ['tx_common_door_nh', 512, 1024],\n",
       " ['tx_common_stone_floor_nh', 1024, 1024],\n",
       " ['tx_doorbricks_01_nh', 256, 512],\n",
       " ['tx_imp_floor_02_nh', 2048, 2048],\n",
       " ['tx_imp_full_01_nh', 1024, 512],\n",
       " ['tx_imp_half_01_nh', 512, 512],\n",
       " ['tx_imp_wall_01_nh', 2048, 2048],\n",
       " ['tx_imp_wall_02_nh', 2048, 2048],\n",
       " ['tx_imp_wall_03_nh', 2048, 2048],\n",
       " ['tx_shingles_01_nh', 1024, 1024],\n",
       " ['tx_v_base_09_nh', 512, 256],\n",
       " ['tx_v_base_10_nh', 512, 1024],\n",
       " ['tx_v_bridgedetail_01_nh', 1024, 1024],\n",
       " ['tx_v_bridgedetail_04_nh', 256, 256],\n",
       " ['tx_v_floor_01_nh', 1024, 1024],\n",
       " ['tx_v_strip_02_nh', 512, 256],\n",
       " ['tx_wall_brick_imp_01_nh', 1024, 1024],\n",
       " ['tx_wall_stone_01_nh', 1024, 1024],\n",
       " ['tx_wg_cobblestones_01_nh', 2048, 2048],\n",
       " ['tx_wg_cobblestone_01_nh', 2048, 2048],\n",
       " ['tx_wood_ceilingpanel_02_nh', 1024, 1024],\n",
       " ['tx_wood_wornfloor_01_nh', 1024, 1024],\n",
       " ['whdirtbrick_n', 4096, 4096],\n",
       " ['whshingle01_n', 2048, 2048],\n",
       " ['whstreetstone01_n', 2048, 2048],\n",
       " ['winterholdfloor01_n', 4096, 4096],\n",
       " ['winterholdfloor02_n', 4096, 4096],\n",
       " ['winterholdgate01_n', 2048, 2048],\n",
       " ['winterholdwall02_n', 4096, 4096],\n",
       " ['woodwall01_n', 4096, 4096],\n",
       " ['wrcitywall01_n', 2048, 2048],\n",
       " ['wrdragontile01_n', 1024, 1024],\n",
       " ['wrfloorboards01_n', 4096, 4096],\n",
       " ['wrtempleboards01_n', 2048, 2048],\n",
       " ['wrtempletiles01_n', 2048, 2048],\n",
       " ['wrwoodfloorint01_n', 4096, 4096]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dirs = ['./Training/Raw_Input_Diffuse/','./Training/Raw_Input_Normal/']\n",
    "target_dirs= ['./Processed_Input_Diffuse/', './Processed_Input_Normal/']\n",
    "channels = ['R','G','B','A']\n",
    "\n",
    "size = 256\n",
    "\n",
    "# This method stores the original sizes of the images.\n",
    "# We do not need this for the first pass, but we will use\n",
    "# it later.\n",
    "\n",
    "def prepare(target,channels):\n",
    "    for i in range(len(channels)):\n",
    "        if path.exists(target[1]+channels[i]):\n",
    "            print(\"====== Using pre-existing resize directory ======\")\n",
    "        else:\n",
    "            os.makedirs(target[1]+channels[i])\n",
    "            \n",
    "def convertAndResizeNormals(data_dir,target,channels):\n",
    "    original_sizes = list()\n",
    "        \n",
    "    for item in os.listdir(data_dir):\n",
    "        print(\"Processing: \"+item)\n",
    "        im=Image.open(data_dir+item)\n",
    "        width,height = im.size\n",
    "        original_sizes.append([item[0:-4],width,height])\n",
    "        for i in range(4):    \n",
    "            single_channel = im.split()[i-1]\n",
    "            single_channel = single_channel.resize((size,size), Image.ANTIALIAS)\n",
    "            single_channel = single_channel.convert(\"RGB\")\n",
    "            \n",
    "            # Drop the last 4 characters, usually the previous filename \".dds\"\n",
    "            single_channel.save(target + channels[i-1] + '/' + item[0:-4] + \".jpg\",\"JPEG\")\n",
    "        \n",
    "        im.close()\n",
    "    \n",
    "    return original_sizes\n",
    "\n",
    "def convertAndResizeDiffuse(data_dir,target):\n",
    "    for item in os.listdir(data_dir):\n",
    "        print(\"Processing: \"+item)\n",
    "        im=Image.open(data_dir+item)\n",
    "        width,height = im.size\n",
    "        im = im.resize((size,size), Image.ANTIALIAS)\n",
    "        im = im.convert(\"RGB\")\n",
    "        # Drop the last 4 characters, usually the previous filename \".dds\"\n",
    "        im.save(target+item[0:-4]+\".jpg\",\"JPEG\")\n",
    "        \n",
    "        im.close()\n",
    "    \n",
    "prepare(target_dirs,channels)\n",
    "convertAndResizeDiffuse(input_dirs[0], target_dirs[0])\n",
    "convertAndResizeNormals(input_dirs[1], target_dirs[1], channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to automatically segment the processed inputs into Test and Train data. Most of our images will be for Training. A few will be held back for testing. \n",
    "\n",
    "The following script will run on both of the Processed Input folders and put a random X% of the images into the testing set, and move the rest of them into the training set. X is 20 by default, but you can change that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./CycleGAN-Tensorflow-2-master/datasets/R-1587222131', './CycleGAN-Tensorflow-2-master/datasets/G-1587222131', './CycleGAN-Tensorflow-2-master/datasets/B-1587222131', './CycleGAN-Tensorflow-2-master/datasets/A-1587222131']\n",
      "['R', 'G', 'B', 'A']\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/R/tx_imp_wall_02_nh.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/R/tx_wood_ceilingpanel_01_nh.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/R/cpstone02_N.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/R/arenastonegrey04_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/R/tx_imp_half_01_nh.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/R/glasswindow10lowerclass_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/R/hhcol03_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/R/tx_bc_muck_nh.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/R/hhsteps01_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/R/tx_wall_brick_imp_01_nh.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/R/arenastonegrey02_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/R/tx_v_base_10_nh.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/R/500wall_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/R/winterholdfloor01_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/R/arenastonegrey01_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/G/tx_imp_wall_02_nh.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/G/riftenstonewall01_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/G/glasswindow10_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/G/wrtempletiles01_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/G/tx_common_door_nh.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/G/riftenstonewall01_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/G/riftenstonewall01_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/G/tx_v_floor_01_nh.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/G/tx_brickedge_left_01_nh.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/G/whshingle01_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/G/shipwoodside03_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/G/wrcitywall01_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/G/cpstone06_N.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/G/stonewall01_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/G/roof01_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/B/sovfloor02_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/B/sovfloor02_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/B/500wall_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/B/winterholdgate01_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/B/glasswindow10lowerclass_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/B/arenastonecolumn01_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/B/skyhavenroof01_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/B/wrwoodfloorint01_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/B/tx_common_stone_floor_nh.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/B/tx_imp_floor_02_nh.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/B/glasswindow10lowerclass_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/B/mrkwalkwaysa01_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/B/hhcol03_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/B/stonewall01_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/B/riftenplazabrick01_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/A/mrkwalkwaysa01_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/A/stonewall01_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/A/tx_doorbricks_01_nh.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/A/hhcol01_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/A/glasswindow10lowerclass_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/A/glasswindow10middleclass_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/A/tx_imp_wall_03_nh.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/A/tx_wg_cobblestones_01_nh.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/A/roof01_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/A/cpstone06_N.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/A/tx_imp_floor_02_nh.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/A/arenastonegrey01_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/A/mrkrockdesigns01_n.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/A/tx_wall_stone_01_nh.jpg\n",
      "78\n",
      "Moving file ./Processed_Input_Normal/A/tx_v_bridgedetail_01_nh.jpg\n",
      "====== Segmentation Complete ======\n"
     ]
    }
   ],
   "source": [
    "percent_to_move = 0.2\n",
    "ahora = str(int(time.time()))\n",
    "\n",
    "from_dirs=['./Processed_Input_Diffuse/', './Processed_Input_Normal/']\n",
    "\n",
    "def generate_to_dirs(GAN_location, channels, ahora):\n",
    "    to_dirs = list()\n",
    "    for i in range(len(channels)):\n",
    "        target = GAN_location+'datasets/'+channels[i]+'-'+ahora\n",
    "        to_dirs.append(target)\n",
    "        os.makedirs(target)\n",
    "    return to_dirs\n",
    "\n",
    "def find_number_of_files(directory):\n",
    "    #print(directory)\n",
    "    count = len([name for name in os.listdir(directory)])\n",
    "    return count \n",
    "\n",
    "    \n",
    "def segment_diffuse(from_dir,to_dirs,percent_to_move):\n",
    "    count = find_number_of_files(from_dir)\n",
    "    num_to_move = int(count * percent_to_move)\n",
    "    #print(count)\n",
    "    #print(num_to_move)\n",
    "    #print (\"Moving \"+str(num_to_move)+\" files to testing directory for\"+from_dir)\n",
    "\n",
    "    # For each color channel, randomly grab images to be test images, then assign\n",
    "    # the rest as training images.\n",
    "    for target in to_dirs:\n",
    "        os.makedirs(target+'/TrainA')\n",
    "        os.makedirs(target+'/TestA')\n",
    "        # Grab the random files and move them to the Test Directory\n",
    "        for i in range(num_to_move):\n",
    "            count = find_number_of_files(from_dir)\n",
    "            items = os.listdir(from_dir)\n",
    "            to_move = items[random.randint(0,count-1)]\n",
    "            #print(\"Moving file \"+to_move)\n",
    "            shutil.copy(from_dir + to_move, target+ '/TestA/' + to_move)\n",
    "        items = os.listdir(from_dir)\n",
    "        for item in items:\n",
    "            shutil.copy(from_dir + item, target+ '/TrainA/' +item)\n",
    "        \n",
    "\n",
    "\n",
    "def segment_normals(from_dir,channels,percent_to_move):\n",
    "    # For each color that we have, we want to move a random selection of files \n",
    "    # to the test directory, and then the rest to the train directory.\n",
    "    for color in channels:\n",
    "        # Figure out where we're putting the files\n",
    "        target = GAN_location + 'datasets/' + color + '-' + ahora + '/TestB/'\n",
    "        \n",
    "        # Make the director that we're going to put stuff in\n",
    "        os.makedirs(target)\n",
    "        \n",
    "        # Figure out exactly how many files we are going to move.\n",
    "        count = find_number_of_files(from_dir + color)\n",
    "        to_move = int(count * percent_to_move)\n",
    "        \n",
    "        # Now we enter the loop to move all of the files up to the percent specified\n",
    "        for i in range(to_move):\n",
    "            # Find how many files we're working with.\n",
    "            count = find_number_of_files(from_dir + color)\n",
    "            print(count)\n",
    "            # Get the list of files.\n",
    "            items = os.listdir(from_dir + color)\n",
    "            # Select a random file to move to the testing directory\n",
    "            to_move = items[random.randint(0,count-1)]\n",
    "            print (\"Moving file \" + from_dir + color +'/' + to_move)\n",
    "            shutil.copy(from_dir + color +'/' + to_move, target + to_move)\n",
    "        \n",
    "        # Finally, we move the remaining images to the training directory.\n",
    "        items = os.listdir(from_dir + color)\n",
    "        target = GAN_location + 'datasets/' + color + '-' + ahora + '/TrainB/'\n",
    "        os.makedirs(target)\n",
    "\n",
    "        for item in items:\n",
    "            shutil.copy(from_dir + color + '/' + item, target + item)\n",
    "    \n",
    "\n",
    "to_dirs = generate_to_dirs(GAN_location, channels, ahora)\n",
    "print(to_dirs)\n",
    "segment_diffuse(from_dirs[0],to_dirs,percent_to_move)\n",
    "print(channels)\n",
    "segment_normals(from_dirs[1], channels, percent_to_move)\n",
    "\n",
    "print( \"====== Segmentation Complete ======\" )\n",
    "# TO DO: Make sure we don't train and test on same files. Make sure we don't copy same files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Two: Network Training\n",
    "\n",
    "You now have prepared data, and can begin training your network. You will set the name of this folder below. This process takes a while.\n",
    "\n",
    "Open a new CMD prompt and CD to the root of the GAN directory - the folder with \"train.py\" and the \"datasets\" folder in it, and run the following command.\n",
    "\n",
    "This process will take a while, but you get a pretty chart to watch while it runs. We can also visualize the results every 100 generations from the Output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train.py --dataset R-1587222131\n",
      "python train.py --dataset G-1587222131\n",
      "python train.py --dataset B-1587222131\n",
      "python train.py --dataset A-1587222131\n"
     ]
    }
   ],
   "source": [
    "# Get the command you need to run in the terminal\n",
    "for color in channels:\n",
    "    print('python train.py --dataset ' + color + '-' + ahora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to view the status of the network via Tensorboard, live. This can give you insight into why your network is performing the way it is. It's also cool looking. Run the following command from within the CycleGAN directory, and point your browser at http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard --logdir output/R-1587222131 --port 6006\n",
      "tensorboard --logdir output/G-1587222131 --port 6006\n",
      "tensorboard --logdir output/B-1587222131 --port 6006\n",
      "tensorboard --logdir output/A-1587222131 --port 6006\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the command you need to run in the terminal\n",
    "for color in channels:\n",
    "    print('tensorboard --logdir output/' + color + '-' + ahora + ' --port 6006')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Three: Normal Generation\n",
    "The next step is to supply the model with input textures and get the normals that you want. This will involve some changes in the underlying code of the GAN, which I will explain later in this notebook.\n",
    "\n",
    "Put the image that you want to generate normals for into the NormalGen/Input folder.\n",
    "\n",
    "We will also have to scale the images back to the original resolution of the input, which requires storing what the resolution is for each input image. That is accomplished in the following block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "targets = ['./NormalGen/Input/', './NormalGen/Output/']\n",
    "input_folder = targets[0]\n",
    "output_folder = targets[1]\n",
    "\n",
    "def prepare():\n",
    "    for item in targets:\n",
    "        if path.exists(item):\n",
    "            print (item + \"directory Exists.\")\n",
    "        else:\n",
    "            os.makedirs(item)\n",
    "            print(\"Created \" + item + \" directory.\")\n",
    "\n",
    "prepare()\n",
    "\n",
    "def convertAndResizeTargets(input_folder,output_folder):\n",
    "    original_sizes = list()\n",
    "    # Find all images that we are attempting to generate normals for.\n",
    "    targets = os.listdir(input_folder)\n",
    "    # Now, convert and resize all of them, and put them in the \"Output\" folder.\n",
    "    for i in range(len(targets)):\n",
    "        im = Image.open(input_folder + targets[i])\n",
    "        width,height = im.size\n",
    "        original_sizes.append([targets[i][0:-4],width,height])\n",
    "        im=im.resize((size,size), Image.ANTIALIAS)\n",
    "        im = im.convert(\"RGB\")\n",
    "        im.save(output_folder + targets[i][0:-4]+\".jpg\",\"JPEG\")\n",
    "    return original_sizes\n",
    "\n",
    "original_sizes = convertAndResizeTargets(input_folder,output_folder)\n",
    "print(original_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we take these processed images and move them to the correct folder, and run the network on them. Since we just trained four networks, we need four copies of each file. \n",
    "\n",
    "We will generate four outputs - one each for each RGBA channel. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in os.listdir(output_folder):\n",
    "    for color in channels:\n",
    "        shutil.copy(output_folder + item, GAN_location+'datasets/'+ color + '-' + ahora + '/TestA/' + item)\n",
    "        print(\"Moving \" + item + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we activate the network with the Train.py that I included in the main folder. It is a very slightly altered version of the \"test.py\" present in the original. The only change is to ensure that it only outputs the single image, rather than the compilation it does by default. Replace the original with the copy supplied. \n",
    "\n",
    "Again, we cannot directly run it. Use the following command from within the GAN folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for color in channels:\n",
    "    print(\"python test.py --experiment_dir ./output/\"+ color +'-' + ahora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this runs, we need to recombine and rescale the output images to match what they were originally. To do this, we will reference the original sizes that we stored earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_location = './Final_Output/'\n",
    "\n",
    "           \n",
    "# for each original image, find all of the components and recombine them.\n",
    "# then rescale them.\n",
    "for entry in original_sizes: \n",
    "    # find all of the original images\n",
    "    # Initialize an empty list which we will store the individual channels in\n",
    "    # prior to merging\n",
    "    im = list()\n",
    "    for color in channels:\n",
    "        gan_output = GAN_location + 'output/' + color + '-' + ahora + '/samples_testing/A2B/'\n",
    "        # We have to convert the image to a luminosity channel before merging.\n",
    "        im.append(Image.open(gan_output + entry[0]+'.jpg').convert(\"L\"))\n",
    "    \n",
    "    # The neural net currently generates alpha channels that are too bright\n",
    "    # and that are lacking contrast. We attempt compensate for this\n",
    "    # by lowering the brightness of all channels by half, and increasing the contrast\n",
    "    # of the alpha channel.\n",
    "    for i in range(len(im)):\n",
    "        enhancer = ImageEnhance.Brightness(im[i])\n",
    "        im[i] = enhancer.enhance(1)\n",
    "    \n",
    "    # I'm messing with the alpha layer here. At this point I'm just fucking around.\n",
    "    im[3] = ImageOps.autocontrast(im[3])\n",
    "    \n",
    "    alpha_brightness = ImageEnhance.Brightness(im[3])\n",
    "    im[3] = alpha_brightness.enhance(.5)\n",
    "    # alpha_contrast = ImageEnhance.Contrast(im[3])\n",
    "    # im[3] = alpha_contrast.enhance(2)\n",
    "    \n",
    "    \n",
    "    # Now that we have all of the channels in a list, we merge\n",
    "    # them back together\n",
    "    print(im)\n",
    "    new_img = Image.merge(\"RGBA\",im)\n",
    "    # Then we resize it to the original size.\n",
    "    new_img = new_img.resize((entry[1],entry[2]), resample=Image.BICUBIC)\n",
    "    # and finally, we save it as a PNG because it keeps transparency that way.\n",
    "    new_img.save(final_output_location + entry[0] +'_nh'+'.png',\"PNG\")\n",
    "\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to convert all of the images to dds. This is accomplished via imagemagick. Go to the output folder and run the following:\n",
    "\n",
    "> magick mogrtify -format dds *\n",
    "\n",
    "Once that's done, go ahead and put them in a folder and add the folder path to your OpenMW cfg as a mod. \n",
    "\n",
    "Congratulations, you're done! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do: \n",
    "\n",
    "- Investigate splitting each image into R, G, B, and A components and creating separate GANs for each, then recombining at the end. The current method loses alpha channel information, which is very important for some of these height or normal maps. See: https://stackoverflow.com/questions/51325224/python-pil-image-split-to-rgb/51555134 for potential way to do this.\n",
    "\n",
    "- Also here for plotting: https://stackoverflow.com/questions/55677216/how-to-convert-an-rgba-image-to-grayscale-in-python\n",
    "\n",
    "- See here for a better way to get the Alpha https://stackoverflow.com/questions/1962795/how-to-get-alpha-value-of-a-png-image-with-pil\n",
    "\n",
    "- Try training another GAN to go from the Alphas that I generate in the first pass, to better Alphas. Perhaps they can be improved...\n",
    "\n",
    "- Find additional training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into each color channel\n",
    "\n",
    "Preliminary investigations are showing me that the transfer from RGBA space to RGB space is, to put it bluntly, awful. It removes a ton of the detail that make textures like Lysol's so nice, and removes an entire dimension from the possible training space. \n",
    "\n",
    "I would like to try out splitting the normals into multiple channels, then training models to generate one channel each. \n",
    "\n",
    "This would involve a refactor of the code a bit. I would need to first split each image, then resize and move around, then create four \"dataset\" dirs, one for each channel. I will use the same input images. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowering Cycle Loss Weight Over Time\n",
    "\n",
    "https://ssnl.github.io/better_cycles/report.pdf See that. Basically, the image is required to be translatable back and forth, which forces the gan to require encoding the necessary information within the image in a way that minimally disrupts the cycle loss weight, but which is still available for it to get back to the original image. It develops its own encoding each time. \n",
    "\n",
    "Keeping the images reversible is a good idea, since it constraints the possible weights, but it can also enforce this lossless encoding. We don't always want it to be lossless. So instead, let's try lowering the cycle loss weight over time. Right now, cycle loss weight has a default value of 10. What if we lower that over time? \n",
    "\n",
    "I'm going to try implementing this. Adjusting Train.py. I have set it so that the cycle loss weight will go scale down linearly throughout the training process, with an end-state of half of the starting state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on a large image corpus\n",
    "\n",
    "I'm interesting in seeing what happens if I train it on an much larger group of images. Most GANs require an incredibly large corpus to perform well. So far, I've been training on the order of tens of images. I want to scale that up to thousands. \n",
    "\n",
    "To test this, I think that I will simply grab _every_ normal that I have, from Morrowind, Oblivion, and Skyrim, and have at it. I will also grab a TON of regular texture files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on larger images. Can my GPU handle them? - NOPE! Tried on 512x512, got out of memory errors. Now trying 300x300, because why not? Looks like it's working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testing on larger batches. Can my GPU handle them?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

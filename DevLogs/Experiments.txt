The purpose of these files is to serve as a record of what I'm doing. 

================================================================================
Experiment: Specific GANs for each Channel
================================================================================
The normal files that I want contain 4 channels of information: R, G, B,
and A. My first attempt at training this network involved removing the alpha
channel and training on only the RGB components. While this produced somewhat
decent results, it ignored an entire dimension of the image. The alpha channel
is what encodes the depth, and is a very important piece of a good normal map.
It is what makes Lysol's maps so spectacular. 

I attempted to fix this by splitting the normal into 4 images that were each a
luminosity channel, and then training 4 separate networks to produce only that
one channel. 

When I wanted to produce the actual normals, I would run each diffuse texture
through each network, and then recombine them. 

This worked, for some small value of the word. It produced technically valid
files, but they were muddy and lacked definition. Part of that is likely due to
the size constraint, but I'm starting to think that, by removing information, I
was hobbling the network. This thing might actually need all of that information
to successfully encode a way to translate diffuse maps into normal maps. 

================================================================================
Experiment: Adjusting the Network to allow for a 4D input 
================================================================================ 
I want the alpha channel to be part of the training, and splitting it out to a
separate file wasn't working. So I figured that it was possible to adjust the
network to operate on 4d tensors, rather than 3d, so that I could pass the
entire image in at once. Perhaps, with all of the information available to it,
the network will be able to better understand how to create the normals. 

It turns out that this is possible. It required some adjustments to the network
code, but it wasn't that hard in the end. I made changes to the train.py,
module.py, and a few of the image library helper files to accept 4-channel input
and produce output as PNGs (which contain an alpha channel).

This worked, and produced superior results to my split-channel attempt. However,
checkerboarding is a huge problem, and the network included some rather strange
"splotching," reminiscent of the holes that I frequently see in early training
generations. It will still require improvement.  


================================================================================
Experiment: Adding a Gradient Penalty Weight
================================================================================
The implementation of CycleGAN that I am using is the canonical one described in
the original CycleGAN paper. The code is on Github, here:
https://github.com/LynnHo/CycleGAN-Tensorflow-2

This ignores some of the improvements that have been made to GANs in the
interim. 

Thankfully, they do include the option to use these improvements in the code, so
I don't need to figure out how to implement them myself. 

With very little idea of which one to try first, I opted to use on the options
for Gradient Penalty Weight. Specifically, I am using the WGAN-GP option. This
option is described here: https://arxiv.org/abs/1704.00028.

================================================================================
FIX: Discriminator Dimensionality
================================================================================
I just realized that the discriminator only had a shape value of (256,256,3), but
the generator makes outputs with a dimensionality of (256,256,4). In effect, I think
that the discriminator was ignoring the alpha channel!

I had updated all of the Conv2D layers in the discriminator to use a
dimensionality of 4, but the first layer was using this argument:

   h = inputs = keras.Input(shape=input_shape)

Which was receiving the incorrect shape. How was alpha information even making
it through? Was it learning to encode it somehow in the first three channels?
That's crazy.    
